{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b656d1fa",
   "metadata": {},
   "source": [
    "### Databases\n",
    "1. Databases\n",
    "Hi and welcome back! In the last chapter, we learned what a data engineer is. In this chapter, you'll learn everything about the tools of a data engineer. Let's start with databases.\n",
    "2. What are databases?\n",
    "Databases are an essential tool for the Data Engineer. They can be used to store information. Before zooming in to the kinds of databases, let's get some definitions out of the way. According to Merriam-Webster, a database is \"a usually large collection of data organized especially for rapid search and retrieval.\" There are few pieces of vital information in this definition. First, the database holds data. Second, databases organize data. We'll see later that there are differences in the level of organization between database types. Lastly, databases help us quickly retrieve or search for data. The database management system or DBMS is usually in charge of this.\n",
    "3. Databases and file storage\n",
    "The main difference between databases and simple storage systems like file systems is the level of organization and the fact that the database management systems abstract away a lot of complicated data operations like search, replication and much more. File systems host less such functionality.\n",
    "4. Structured and unstructured data\n",
    "In the universe of databases, there's a big difference in the level of organization. To understand these differences, we have to make a distinction between structured and unstructured data. On one hand, structured data is coherent to a well-defined structure. Database schemas usually define such structure. An example of structured data is tabular data in a relational database. Unstructured data, on the other hand, is schemaless. It looks a lot more like files. Unstructured data could be something like photographs or videos. Structured and unstructured data define outer boundaries, and there is a whole lot of semi-structured data in between. An example of semi-structured data is JSON data.\n",
    "5. SQL and NoSQL\n",
    "Another distinction we can make is the one between SQL and NoSQL. Generally speaking, in SQL databases, tables form the data. The database schema defines relations between these tables. We call SQL databases relational. For example, we could create one table for customers and another for orders. The database schema defines the relationships and properties. Typical SQL databases are MySQL and PostgreSQL. On the other hand, NoSQL databases are called non-relational. NoSQL is often associated with unstructured, schemaless data. That's a misconception, as there are several types of NoSQL databases and they are not all unstructured. Two highly used NoSQL database types are key-value stores like Redis or document databases like MongoDB. In key-value stores, the values are simple. Typical use cases are caching or distributed configuration. Values in a document database are structured or semi-structured objects, for example, a JSON object.\n",
    "6. SQL: The database schema\n",
    "For the remainder of this video, let's focus on database schemas. A schema describes the structure and relations of a database. In this slide, you can see a database schema on the left-hand side. It represents the relations shown in the diagram to the right. We see a table called Customer and one called Order. The column called customer_id connects orders with customers. We call this kind of column a foreign key, as it refers to another table. The SQL statements on the left create the tables of the schema. As you've seen in courses on SQL, you can leverage these foreign keys by joining tables using the JOIN statement.\n",
    "7. SQL: Star schema\n",
    "In data warehousing, a schema you'll see often is the star schema. A lot of analytical databases like Redshift have optimizations for these kinds of schemas. According to Wikipedia, \"the star schema consists of one or more fact tables referencing any number of dimension tables.\" Fact tables contain records that represent things that happened in the world, like orders. Dimension tables hold information on the world itself, like customer names or product prices.\n",
    "1 Wikipedia: https://en.wikipedia.org/wiki/Star_schema\n",
    "8. Let's practice!\n",
    "Now that you know about databases, let's practice in the exercises!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92e877c",
   "metadata": {},
   "source": [
    "### What is parallel computing?\n",
    "1. What is parallel computing\n",
    "Hi again! Now that you've learned everything about databases let's talk about parallel computing. In data engineering, you often have to pull in data from several sources and join them together, clean them, or aggregate them. In this video, we'll see how this is possible for massive amounts of data.\n",
    "2. Idea behind parallel computing\n",
    "Before we go into the different kinds of tools that exist in the data engineering ecosystem, it's crucial to understand the concept of parallel computing. Parallel computing forms the basis of almost all modern data processing tools. However, why has it become so important in the world of big data? The main reason is memory and processing power, but mostly memory. When big data processing tools perform a processing task, they split it up into several smaller subtasks. The processing tools then distribute these subtasks over several computers. These are usually commodity computers, which means they are widely available and relatively inexpensive. Individually, all of the computers would take a long time to process the complete task. However, since all the computers work in parallel on smaller subtasks, the task in its whole is done faster.\n",
    "3. The tailor shop\n",
    "Let's look at an analogy. Let's say you're running a tailor shop and need to get a batch of 100 shirts finished. Your very best tailor finishes a shirt in 20 minutes. Other tailors typically take 1 hour per shirt. If just one tailor can work at a time, it's obvious you'd have to choose the quickest tailor to finish the job. However, if you can split the batch in 25 shirts each, having 4 mediocre tailors working in parallel is faster. A similar thing happens for big data processing tasks.\n",
    "4. Benefits of parallel computing\n",
    "As you'd expect, the obvious benefit of having multiple processing units is the extra processing power itself. However, there is another, and potentially more impactful benefit of parallel computing for big data. Instead of needing to load all of the data in one computer's memory, you can partition the data and load the subsets into memory of different computers. That means the memory footprint per computer is relatively small, and the data can fit in the memory closest to the processor, the RAM.\n",
    "5. Risks of parallel computing\n",
    "Before you start rewriting all your code to use parallel computing, keep in mind that this also comes at its cost. Splitting a task into subtask and merging the results of the subtasks back into one final result requires some communication between processes. This communication overhead can become a bottleneck if the processing requirements are not substantial, or if you have too little processing units. In other words, if you have 2 processing units, a task that takes a few hundred milliseconds might not be worth splitting up. Additionally, due to the overhead, the speed does not increase linearly. This effect is also called parallel slowdown.\n",
    "6. An example\n",
    "Let's look into a more practical example. We're starting with a dataset of all Olympic events from 1896 until 2016. From this dataset, you want to get an average age of participants for each year. For this example, let's say you have four processing units at your disposal. You decide to distribute the load over all of your processing units. To do so, you need to split the task into smaller subtasks. In this example, the average age calculation for each group of years is as a subtask. You can achieve that through 'groupby.' Then, you distribute all of these subtasks over the four processing units. This example illustrates roughly how the first distributed algorithms like Hadoop MapReduce work, the difference being the processing units are distributed over several machines.\n",
    "7. multiprocessing.Pool\n",
    "In code, there are several ways of implementing this. At a low level, we could use the `multiprocessing.Pool` API to distribute work over several cores on the same machine. Let's say we have a function `take_mean_age`, which accepts a tuple: the year of the group and the group itself as a DataFrame. `take_mean_age` returns a DataFrame with one observation and one column: the mean age of the group. The resulting DataFrame is indexed by year. We can then take this function, and map it over the groups generated by `.groupby()`, using the `.map()` method of `Pool`. By defining `4` as an argument to `Pool`, the mapping runs in 4 separate processes, and thus uses 4 cores. Finally, we can concatenate the results to form the resulting DataFrame.\n",
    "8. dask\n",
    "Several packages offer a layer of abstraction to avoid having to write such low-level code. For example, the `dask` framework offers a DataFrame object, which performs a groupby and apply using multiprocessing out of the box. You need to define the number of partitions, for example, `4`. `dask` divides the DataFrame into 4 parts, and performs `.mean()` within each part separately. Because `dask` uses lazy evaluation, you need to add `.compute()` to the end of the chain.\n",
    "9. Let's practice!\n",
    "That was the final example of this video. In the exercises, you'll use the packages yourself. Good luck!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3756a2f7",
   "metadata": {},
   "source": [
    "### From task to subtasks\n",
    "For this exercise, you will be using parallel computing to apply the function take_mean_age() that calculates the average athlete's age in a given year in the Olympics events dataset. The DataFrame athlete_events has been loaded for you and contains amongst others, two columns:\n",
    "\n",
    "Year: the year the Olympic event took place\n",
    "Age: the age of the Olympian\n",
    "You will be using the multiprocessor.Pool API which allows you to distribute your workload over several processes. The function parallel_apply() is defined in the sample code. It takes in as input the function being applied, the grouping used, and the number of cores needed for the analysis. Note that the @print_timing decorator is used to time each operation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1013b2f8",
   "metadata": {},
   "source": [
    "Complete the code, so you apply take_mean_age with 1 core first, then 2 and finally 4 cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a92c8c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'print_timing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Function to apply a function over multiple cores\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;129m@print_timing\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_apply\u001b[39m(apply_func, groups, nb_cores):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Pool(nb_cores) \u001b[38;5;28;01mas\u001b[39;00m p:\n\u001b[0;32m      5\u001b[0m         results \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mmap(apply_func, groups)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'print_timing' is not defined"
     ]
    }
   ],
   "source": [
    "# Function to apply a function over multiple cores\n",
    "@print_timing\n",
    "def parallel_apply(apply_func, groups, nb_cores):\n",
    "    with Pool(nb_cores) as p:\n",
    "        results = p.map(apply_func, groups)\n",
    "    return pd.concat(results)\n",
    " \n",
    "# Parallel apply using 1 core\n",
    "parallel_apply(take_mean_age, athlete_events.groupby('Year'), 1)\n",
    " \n",
    "# Parallel apply using 2 cores\n",
    "parallel_apply(take_mean_age, athlete_events.groupby('Year'), 2)\n",
    " \n",
    "# Parallel apply using 4 cores\n",
    "parallel_apply(take_mean_age, athlete_events.groupby('Year'), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430a2cf9",
   "metadata": {},
   "source": [
    "### Using a DataFrame\n",
    "In the previous exercise, you saw how to split up a task and use the low-level python multiprocessing.Pool API to do calculations on several processing units.\n",
    "\n",
    "It's essential to understand this on a lower level, but in reality, you'll never use this kind of APIs. A more convenient way to parallelize an apply over several groups is using the dask framework and its abstraction of the pandas DataFrame, for example.\n",
    "\n",
    "The pandas DataFrame, athlete_events, is available in your workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af63475",
   "metadata": {},
   "source": [
    "Create 4 partitions of the athletes_events DataFrame using dd.from_pandas().\n",
    "If you forgot the parameters of dd.from_pandas(), check out the slides again, or type help(dd.from_pandas) in the console!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1da4b54b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'athlete_events' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataframe\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Set the number of partitions\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m athlete_events_dask \u001b[38;5;241m=\u001b[39m dd\u001b[38;5;241m.\u001b[39mfrom_pandas(\u001b[43mathlete_events\u001b[49m, npartitions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'athlete_events' is not defined"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Set the number of partitions\n",
    "athlete_events_dask = dd.from_pandas(athlete_events, npartitions=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4c06dc",
   "metadata": {},
   "source": [
    "Print out the mean age for each Year. Remember dask uses lazy evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4193b987",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'athlete_events' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataframe\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Set the number of partitions\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m athlete_events_dask \u001b[38;5;241m=\u001b[39m dd\u001b[38;5;241m.\u001b[39mfrom_pandas(\u001b[43mathlete_events\u001b[49m, npartitions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Calculate the mean Age per Year\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(athlete_events_dask\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYear\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mAge\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mcompute())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'athlete_events' is not defined"
     ]
    }
   ],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Set the number of partitions\n",
    "athlete_events_dask = dd.from_pandas(athlete_events, npartitions=4)\n",
    "\n",
    "# Calculate the mean Age per Year\n",
    "print(athlete_events_dask.groupby('Year').Age.mean().compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5a8a75",
   "metadata": {},
   "source": [
    "### Parallel computation frameworks\n",
    "1. Parallel computation frameworks\n",
    "Hi again. Now that we've seen the basics of parallel computing, it's time to talk about specific parallel computing frameworks. We'll focus on parallel computing frameworks that are currently hot in the data engineering world.\n",
    "2. Hadoop\n",
    "Before we dive into state of the art big data processing tools, let's talk a bit about the foundations. If you've done some investigation into big data ecosystems, you've probably heard about Hadoop. Hadoop is a collection of open source projects, maintained by the Apache Software Foundation. Some of them are a bit outdated, but it's still relevant to talk about them. There are two Hadoop projects we want to focus on for this video: MapReduce and HDFS.\n",
    "3. HDFS\n",
    "First, HDFS is a distributed file system. It's similar to the file system you have on your computer, the only difference being the files reside on multiple different computers. HDFS has been essential in the big data world, and for parallel computing by extension. Nowadays, cloud-managed storage systems like Amazon S3 often replace HDFS.\n",
    "4. MapReduce\n",
    "Second, MapReduce was one of the first popularized big-data processing paradigms. It works similar to the example you saw in the previous video, where the program splits tasks into subtasks, distributing the workload and data between several processing units. For MapReduce, these processing units are several computers in the cluster. MapReduce had its flaws; one of it was that it was hard to write these MapReduce jobs. Many software programs popped up to address this problem, and one of them was Hive.\n",
    "5. Hive\n",
    "Hive is a layer on top of the Hadoop ecosystem that makes data from several sources queryable in a structured way using Hive's SQL variant: Hive SQL. Facebook initially developed Hive, but the Apache Software Foundation now maintains the project. Although MapReduce was initially responsible for running the Hive jobs, it now integrates with several other data processing tools.\n",
    "6. Hive: an example\n",
    "Let's look at an example: this Hive query selects the average age of the Olympians per Year they participated. As you'd expect, this query looks indistinguishable from a regular SQL query. However, behind the curtains, this query is transformed into a job that can operate on a cluster of computers.\n",
    "7. Spark\n",
    "The other parallel computation framework we'll introduce is called Spark. Spark distributes data processing tasks between clusters of computers. While MapReduce-based systems tend to need expensive disk writes between jobs, Spark tries to keep as much processing as possible in memory. In that sense, Spark was also an answer to the limitations of MapReduce. The disk writes of MapReduce were especially limiting in interactive exploratory data analysis, where each step builds on top of a previous step. Spark originates from the University of California, where it was developed at the Berkeley's AMPLab. Currently, the Apache Software Foundation maintains the project.\n",
    "8. Resilient distributed datasets (RDD)\n",
    "Spark's architecture relies on something called resilient distributed datasets, or RDDs. Without diving into technicalities, this is a data structure that maintains data which is distributed between multiple nodes. Unlike DataFrames, RDDs don't have named columns. From a conceptual perspective, you can think of RDDs as lists of tuples. We can do two types of operations on these data structures: transformations, like map or filter, and actions, like count or first. Transformations result in transformed RDDs, while actions result in a single result.\n",
    "9. PySpark\n",
    "When working with Spark, people typically use a programming language interface like PySpark. PySpark is the Python interface to spark. There are interfaces to Spark in other languages, like R or Scala, as well. PySpark hosts a DataFrame abstraction, which means that you can do operations very similar to pandas DataFrames. PySpark and Spark take care of all the complex parallel computing operations.\n",
    "10. PySpark: an example\n",
    "Have a look at the following PySpark example. Similar to the Hive Query you saw before, it calculates the mean age of the Olympians, per Year of the Olympic event. Instead of using the SQL abstraction, like in the Hive Example, it uses the DataFrame abstraction.\n",
    "11. Let's practice!\n",
    "Let's try this in the exercises!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2600ec14",
   "metadata": {},
   "source": [
    "### A PySpark groupby\n",
    "You've seen how to use the dask framework and its DataFrame abstraction to do some calculations. However, as you've seen in the video, in the big data world Spark is probably a more popular choice for data processing.\n",
    "\n",
    "In this exercise, you'll use the PySpark package to handle a Spark DataFrame. The data is the same as in previous exercises: participants of Olympic events between 1896 and 2016.\n",
    "\n",
    "The Spark Dataframe, athlete_events_spark is available in your workspace.\n",
    "\n",
    "The methods you're going to use in this exercise are:\n",
    "\n",
    ".printSchema(): helps print the schema of a Spark DataFrame.\n",
    ".groupBy(): grouping statement for an aggregation.\n",
    ".mean(): take the mean over each group.\n",
    ".show(): show the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20d3ef3",
   "metadata": {},
   "source": [
    "Find out the type of athlete_events_spark.\n",
    "Find out the schema of athlete_events_spark.\n",
    "Print out the mean age of the Olympians, grouped by year. Notice that spark has not actually calculated anything yet. You can call this lazy evaluation.\n",
    "Take the previous result, and call .show() on the result to calculate the mean age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e2da656",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'athlete_events_spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Print the type of athlete_events_spark\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(\u001b[43mathlete_events_spark\u001b[49m))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Print the schema of athlete_events_spark\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(athlete_events_spark\u001b[38;5;241m.\u001b[39mprintSchema())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'athlete_events_spark' is not defined"
     ]
    }
   ],
   "source": [
    "# Print the type of athlete_events_spark\n",
    "print(type(athlete_events_spark))\n",
    "\n",
    "# Print the schema of athlete_events_spark\n",
    "print(athlete_events_spark.printSchema())\n",
    "\n",
    "# Group by the Year, and find the mean Age\n",
    "print(athlete_events_spark.groupBy('Year').mean('Age'))\n",
    "\n",
    "# Group by the Year, and find the mean Age\n",
    "print(athlete_events_spark.groupBy('Year').mean('Age').show())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9773442d",
   "metadata": {},
   "source": [
    "### Running PySpark files\n",
    "In this exercise, you're going to run a PySpark file using spark-submit. This tool can help you submit your application to a spark cluster.\n",
    "\n",
    "For the sake of this exercise, you're going to work with a local Spark instance running on 4 threads. The file you need to submit is in /home/repl/spark-script.py. Feel free to read the file:\n",
    "\n",
    "cat /home/repl/spark-script.py\n",
    "You can use spark-submit as follows:\n",
    "\n",
    "spark-submit \\\n",
    "  --master local[4] \\\n",
    "  /home/repl/spark-script.py\n",
    "What does this output? Note that it may take a few seconds to get your results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfe17d8",
   "metadata": {},
   "source": [
    "##### Answer\n",
    "\n",
    "spark-submit \\\n",
    "  --master local[4] \\\n",
    "  /home/repl/spark-script.py\n",
    "\n",
    "A DataFrame with average Olympian heights by year."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9423ca",
   "metadata": {},
   "source": [
    "### Workflow scheduling frameworks\n",
    "1. Workflow scheduling frameworks\n",
    "Hi, and welcome to the last video of the chapter. In this video, we'll introduce the last tool we'll talk about in this course. These are the workflow scheduling frameworks. We've seen how to pull data from existing databases. We've also seen parallel computing frameworks like Spark. However, something needs to put all of these jobs together. It's the task of the workflow scheduling framework to orchestrate these jobs.\n",
    "2. An example pipeline\n",
    "Let's take an example. You can write a Spark job that pulls data from a CSV file, filters out some corrupt records, and loads the data into a SQL database ready for analysis. However, let's say you need to do this every day as new data is coming in to the CSV file. One option is to run the job each day manually. Of course, that doesn't scale well: what about the weekends? There are simple tools that could solve this problem, like cron, the Linux tool. However, let's say you have one job for the CSV file and another job to pull in and clean the data from an API, and a third job that joins the data from the CSV and the API together. The third job depends on the first two jobs to finish. It quickly becomes apparent that we need a more holistic approach, and a simple tool like cron won't suffice.\n",
    "3. DAGs\n",
    "So, we ended up with dependencies between jobs. A great way to visualize these dependencies is through Directed Acyclic Graphs, or DAGs. A DAG is a set of nodes that are connected by directed edges. There are no cycles in the graph, which means that no path following the directed edges sees a specific node more than once. In the example on the slide, Job A needs to happen first, then Job B, which enables Job C and D and finally Job E. As you can see, it feels natural to represent this kind of workflow in a DAG. The jobs represented by the DAG can then run in a daily schedule, for example.\n",
    "4. The tools for the job\n",
    "So, we've talked about dependencies and scheduling DAGs, what tools are there to use for us? Well, first of all, some people use the Linux tool, cron. However, most companies use a more full-fledged solution. There's Spotify's Luigi, which allows for the definition of DAGs for complex pipelines. However, for the remainder of the video, we'll focus on Apache Airflow. Airflow is growing out to be the de-facto workflow scheduling framework.\n",
    "5. Apache Airflow\n",
    "Airbnb created Airflow as an internal tool for workflow management. They open-sourced Airflow in 2015, and it later joined the Apache Software Foundation in 2016. They built Airflow around the concept of DAGs. Using Python, developers can create and test these DAGs that build up complex pipelines.\n",
    "6. Airflow: an example DAG\n",
    "Let's look at an example from an e-commerce use-case in the DAG showed on the slide. The first job starts a Spark cluster. Once it's started, we can pull in customer and product data by running the ingest_customer_data and ingest_product_data jobs. Finally, we aggregate both tables using the enrich_customer_data job which runs after both ingest_customer_data and ingest_product_data complete.\n",
    "7. Airflow: an example in code\n",
    "In code, it would look something like this. First, we create a DAG using the `DAG` class. Afterward, we use an Operator to define each of the jobs. Several kinds of operators exist in Airflow. There are simple ones like BashOperator and PythonOperator that execute bash or Python code, respectively. Then there are ways to write your own operator, like the SparkJobOperator or StartClusterOperator in the example. Finally, we define the connections between these operators using `.set_downstream()`.\n",
    "8. Let's practice!\n",
    "Ok, let's see how you do in the exercises.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e59fc37",
   "metadata": {},
   "source": [
    "### Airflow DAGs\n",
    "In Airflow, a pipeline is represented as a Directed Acyclic Graph or DAG. The nodes of the graph represent tasks that are executed. The directed connections between nodes represent dependencies between the tasks.\n",
    "\n",
    "Representing a data pipeline as a DAG makes much sense, as some tasks need to finish before others can start. You could compare this to an assembly line in a car factory. The tasks build up, and each task can depend on previous tasks being finished. A fictional DAG could look something like this:\n",
    "\n",
    "Example DAG\n",
    "\n",
    "Assembling the frame happens first, then the body and tires and finally you paint. Let's reproduce the example above in code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9bee37",
   "metadata": {},
   "source": [
    "First, the DAG needs to run on every hour at minute 0. Fill in the schedule_interval keyword argument using the crontab notation. For example, every hour at minute N would be N * * * *. Remember, you need to run at minute 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65473f19",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DAG' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create the DAG object\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m dag \u001b[38;5;241m=\u001b[39m \u001b[43mDAG\u001b[49m(dag_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcar_factory_simulation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m           default_args\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mowner\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mairflow\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart_date\u001b[39m\u001b[38;5;124m\"\u001b[39m: airflow\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdates\u001b[38;5;241m.\u001b[39mdays_ago(\u001b[38;5;241m2\u001b[39m)},\n\u001b[0;32m      4\u001b[0m           schedule_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0 * * * *\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DAG' is not defined"
     ]
    }
   ],
   "source": [
    "# Create the DAG object\n",
    "dag = DAG(dag_id=\"car_factory_simulation\",\n",
    "          default_args={\"owner\": \"airflow\",\"start_date\": airflow.utils.dates.days_ago(2)},\n",
    "          schedule_interval=\"0 * * * *\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a601aee",
   "metadata": {},
   "source": [
    "The downstream flow should match what you can see in the image above. The first step has already been filled in for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ebbb09f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DAG' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create the DAG object\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m dag \u001b[38;5;241m=\u001b[39m \u001b[43mDAG\u001b[49m(dag_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcar_factory_simulation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m           default_args\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mowner\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mairflow\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart_date\u001b[39m\u001b[38;5;124m\"\u001b[39m: airflow\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdates\u001b[38;5;241m.\u001b[39mdays_ago(\u001b[38;5;241m2\u001b[39m)},\n\u001b[0;32m      4\u001b[0m           schedule_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0 * * * *\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Task definitions\u001b[39;00m\n\u001b[0;32m      7\u001b[0m assemble_frame \u001b[38;5;241m=\u001b[39m BashOperator(task_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massemble_frame\u001b[39m\u001b[38;5;124m\"\u001b[39m, bash_command\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mecho \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAssembling frame\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m, dag\u001b[38;5;241m=\u001b[39mdag)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DAG' is not defined"
     ]
    }
   ],
   "source": [
    "# Create the DAG object\n",
    "dag = DAG(dag_id=\"car_factory_simulation\",\n",
    "          default_args={\"owner\": \"airflow\",\"start_date\": airflow.utils.dates.days_ago(2)},\n",
    "          schedule_interval=\"0 * * * *\")\n",
    "\n",
    "# Task definitions\n",
    "assemble_frame = BashOperator(task_id=\"assemble_frame\", bash_command='echo \"Assembling frame\"', dag=dag)\n",
    "place_tires = BashOperator(task_id=\"place_tires\", bash_command='echo \"Placing tires\"', dag=dag)\n",
    "assemble_body = BashOperator(task_id=\"assemble_body\", bash_command='echo \"Assembling body\"', dag=dag)\n",
    "apply_paint = BashOperator(task_id=\"apply_paint\", bash_command='echo \"Applying paint\"', dag=dag)\n",
    "\n",
    "# Complete the downstream flow\n",
    "assemble_frame.set_downstream(place_tires)\n",
    "assemble_frame.set_downstream(assemble_body)\n",
    "assemble_body.set_downstream(apply_paint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b87efe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
